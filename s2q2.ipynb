{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "\n",
    "# psycopg2 for interacting with postgres\n",
    "try:\n",
    "    import psycopg2 as pg\n",
    "    import psycopg2.extras\n",
    "except:\n",
    "    print( \"Install psycopg2\")\n",
    "    exit(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5996996\n",
      "0 rows have been loaded to dataframe.\n",
      "50000 rows have been loaded to dataframe.\n",
      "100000 rows have been loaded to dataframe.\n",
      "150000 rows have been loaded to dataframe.\n",
      "200000 rows have been loaded to dataframe.\n",
      "250000 rows have been loaded to dataframe.\n",
      "300000 rows have been loaded to dataframe.\n",
      "350000 rows have been loaded to dataframe.\n",
      "400000 rows have been loaded to dataframe.\n",
      "450000 rows have been loaded to dataframe.\n",
      "500000 rows have been loaded to dataframe.\n",
      "550000 rows have been loaded to dataframe.\n",
      "600000 rows have been loaded to dataframe.\n",
      "650000 rows have been loaded to dataframe.\n",
      "700000 rows have been loaded to dataframe.\n",
      "750000 rows have been loaded to dataframe.\n",
      "800000 rows have been loaded to dataframe.\n",
      "850000 rows have been loaded to dataframe.\n",
      "900000 rows have been loaded to dataframe.\n",
      "950000 rows have been loaded to dataframe.\n",
      "1000000 rows have been loaded to dataframe.\n",
      "1050000 rows have been loaded to dataframe.\n",
      "1100000 rows have been loaded to dataframe.\n",
      "1150000 rows have been loaded to dataframe.\n",
      "1200000 rows have been loaded to dataframe.\n",
      "1250000 rows have been loaded to dataframe.\n",
      "1300000 rows have been loaded to dataframe.\n",
      "1350000 rows have been loaded to dataframe.\n",
      "1400000 rows have been loaded to dataframe.\n",
      "1450000 rows have been loaded to dataframe.\n",
      "1500000 rows have been loaded to dataframe.\n",
      "1550000 rows have been loaded to dataframe.\n",
      "1600000 rows have been loaded to dataframe.\n",
      "1650000 rows have been loaded to dataframe.\n",
      "1700000 rows have been loaded to dataframe.\n",
      "1750000 rows have been loaded to dataframe.\n",
      "1800000 rows have been loaded to dataframe.\n",
      "1850000 rows have been loaded to dataframe.\n",
      "1900000 rows have been loaded to dataframe.\n",
      "1950000 rows have been loaded to dataframe.\n",
      "2000000 rows have been loaded to dataframe.\n",
      "2050000 rows have been loaded to dataframe.\n",
      "2100000 rows have been loaded to dataframe.\n",
      "2150000 rows have been loaded to dataframe.\n",
      "2200000 rows have been loaded to dataframe.\n",
      "2250000 rows have been loaded to dataframe.\n",
      "2300000 rows have been loaded to dataframe.\n",
      "2350000 rows have been loaded to dataframe.\n",
      "2400000 rows have been loaded to dataframe.\n",
      "2450000 rows have been loaded to dataframe.\n",
      "2500000 rows have been loaded to dataframe.\n",
      "2550000 rows have been loaded to dataframe.\n",
      "2600000 rows have been loaded to dataframe.\n",
      "2650000 rows have been loaded to dataframe.\n",
      "2700000 rows have been loaded to dataframe.\n",
      "2750000 rows have been loaded to dataframe.\n",
      "2800000 rows have been loaded to dataframe.\n",
      "2850000 rows have been loaded to dataframe.\n",
      "2900000 rows have been loaded to dataframe.\n",
      "2950000 rows have been loaded to dataframe.\n",
      "3000000 rows have been loaded to dataframe.\n",
      "3050000 rows have been loaded to dataframe.\n",
      "3100000 rows have been loaded to dataframe.\n",
      "3150000 rows have been loaded to dataframe.\n",
      "3200000 rows have been loaded to dataframe.\n",
      "3250000 rows have been loaded to dataframe.\n",
      "3300000 rows have been loaded to dataframe.\n",
      "3350000 rows have been loaded to dataframe.\n",
      "3400000 rows have been loaded to dataframe.\n",
      "3450000 rows have been loaded to dataframe.\n",
      "3500000 rows have been loaded to dataframe.\n",
      "3550000 rows have been loaded to dataframe.\n",
      "3600000 rows have been loaded to dataframe.\n",
      "3650000 rows have been loaded to dataframe.\n",
      "3700000 rows have been loaded to dataframe.\n",
      "3750000 rows have been loaded to dataframe.\n",
      "3800000 rows have been loaded to dataframe.\n",
      "3850000 rows have been loaded to dataframe.\n",
      "3900000 rows have been loaded to dataframe.\n",
      "3950000 rows have been loaded to dataframe.\n",
      "4000000 rows have been loaded to dataframe.\n",
      "4050000 rows have been loaded to dataframe.\n",
      "4100000 rows have been loaded to dataframe.\n",
      "4150000 rows have been loaded to dataframe.\n",
      "4200000 rows have been loaded to dataframe.\n",
      "4250000 rows have been loaded to dataframe.\n",
      "4300000 rows have been loaded to dataframe.\n",
      "4350000 rows have been loaded to dataframe.\n",
      "4400000 rows have been loaded to dataframe.\n",
      "4450000 rows have been loaded to dataframe.\n",
      "4500000 rows have been loaded to dataframe.\n",
      "4550000 rows have been loaded to dataframe.\n",
      "4600000 rows have been loaded to dataframe.\n",
      "4650000 rows have been loaded to dataframe.\n",
      "4700000 rows have been loaded to dataframe.\n",
      "4750000 rows have been loaded to dataframe.\n",
      "4800000 rows have been loaded to dataframe.\n",
      "4850000 rows have been loaded to dataframe.\n",
      "4900000 rows have been loaded to dataframe.\n",
      "4950000 rows have been loaded to dataframe.\n",
      "5000000 rows have been loaded to dataframe.\n",
      "5050000 rows have been loaded to dataframe.\n",
      "5100000 rows have been loaded to dataframe.\n",
      "5150000 rows have been loaded to dataframe.\n",
      "5200000 rows have been loaded to dataframe.\n",
      "5250000 rows have been loaded to dataframe.\n",
      "5300000 rows have been loaded to dataframe.\n",
      "5350000 rows have been loaded to dataframe.\n",
      "5400000 rows have been loaded to dataframe.\n",
      "5450000 rows have been loaded to dataframe.\n",
      "5500000 rows have been loaded to dataframe.\n",
      "5550000 rows have been loaded to dataframe.\n",
      "5600000 rows have been loaded to dataframe.\n",
      "5650000 rows have been loaded to dataframe.\n",
      "5700000 rows have been loaded to dataframe.\n",
      "5750000 rows have been loaded to dataframe.\n",
      "5800000 rows have been loaded to dataframe.\n",
      "5850000 rows have been loaded to dataframe.\n",
      "5900000 rows have been loaded to dataframe.\n",
      "5950000 rows have been loaded to dataframe.\n"
     ]
    }
   ],
   "source": [
    "PG_CONN_STRING = \"dbname='postgres' port='5432' user='postgres' password='phludphlud'\"\n",
    "dbconn = pg.connect(PG_CONN_STRING)\n",
    "cursor = dbconn.cursor()\n",
    "\n",
    "row_count = int(pd.read_sql('SELECT COUNT(*) from review_view', con=dbconn).values)\n",
    "\n",
    "print(row_count)\n",
    "chunksize = 50000\n",
    "review_df = pd.DataFrame(columns = ['text', 'stars'])\n",
    "\n",
    "# Load review into Pandas DataFrame\n",
    "for i in range(int(row_count/chunksize) +1):\n",
    "    query = 'SELECT text, stars from review_view LIMIT {chunksize} OFFSET {offset}'.format(offset = i*chunksize, chunksize=chunksize)\n",
    "    review_df = review_df.append(pd.read_sql_query(query, con=dbconn))\n",
    "    print(\"{} rows have been loaded to dataframe.\".format(i*chunksize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text stars\n",
      "0  Its a shame the staff Dr Agarwal chooses to su...     1\n",
      "1  I purchased my phone October 2009 and have bee...     1\n",
      "2  I am originally from NY and grew up eating Spa...     5\n",
      "3  Don't waste your money getting an interior and...     2\n",
      "4  On Saturday November 24th, 2012, we visited ou...     1\n",
      "(5996996, 2)\n"
     ]
    }
   ],
   "source": [
    "# Quick preview of the data\n",
    "print(review_df.head()) \n",
    "print(review_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multi-label classifier might be difficult in this case. In future iterations we may test or even train for this, but let's only use most positive and negative reviews for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     object\n",
      "stars     int32\n",
      "dtype: object\n",
      "(3494625, 2)\n"
     ]
    }
   ],
   "source": [
    "review_df['stars'] = review_df['stars'].astype(int)\n",
    "print(review_df.dtypes)\n",
    "filtered_df = review_df[review_df['stars'].isin([1,5])]\n",
    "\n",
    "print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert stars to a 0-1 scale, which will make it allow us to compare to sigmoid outputs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  stars\n",
      "0  Its a shame the staff Dr Agarwal chooses to su...      0\n",
      "1  I purchased my phone October 2009 and have bee...      0\n",
      "2  I am originally from NY and grew up eating Spa...      1\n",
      "4  On Saturday November 24th, 2012, we visited ou...      0\n",
      "6  I've been to this Burger King between 5-10 tim...      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JG\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to 0-1 scale (1 star becomes 0, 5 star becomes 1)\n",
    "def convert_scale(x):    \n",
    "    if x == 1: x = 0\n",
    "    if x == 5: x = 1\n",
    "    return x\n",
    "\n",
    "filtered_df['stars'] = filtered_df['stars'].apply(convert_scale)\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the reviews took too long since I'm training on a CPU, so let's take only 10% of the total filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1 and 5 star reviews: (3494625, 2)\n",
      "(349462, 2)\n",
      "                                                    text  stars\n",
      "25676  This place is beautifully decorated and very w...      1\n",
      "25678  Sherrill came to my rescue after a previous ha...      1\n",
      "25679  I have been going here for a little over four ...      1\n",
      "25680  So we walked I today at 2:20 and my appointmen...      0\n",
      "25681  I stayed here on 2/10/17.   I couldn't find th...      0\n"
     ]
    }
   ],
   "source": [
    "print(\"All 1 and 5 star reviews:\", filtered_df.shape)\n",
    "x = int(filtered_df.shape[0]/10)\n",
    "\n",
    "print(filtered_df.iloc[0:x].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to .csv format which is compatible with TorchText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.iloc[0:x].to_csv(\"filtered2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch will be used for building our model since it has some nice libraries for NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data, datasets\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Based on bi-grams concept from \"FastText\" model (Joulin et al., 2016)\n",
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy', preprocessing=generate_bigrams)\n",
    "LABEL = data.LabelField(tensor_type=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [(\"text\", TEXT), (\"stars\", LABEL)]\n",
    "train = data.TabularDataset(\n",
    "        path = 'filtered2.csv', format='csv', skip_header=True, fields=datafields) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training, validation and test sets, and then generate vocabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 342473\n",
      "Valid length: 3494\n",
      "Test length: 3495\n",
      "Length of TEXT vocab: 25002\n",
      "Length of LABEL vocab: 2\n",
      "Iterators generated.\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = train.split(split_ratio=[0.98, 0.01, 0.01])\n",
    "\n",
    "print('Train length:', len(train))\n",
    "print('Valid length:', len(valid))\n",
    "print('Test length:', len(test))\n",
    "\n",
    "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "print(\"Length of TEXT vocab:\", len(TEXT.vocab))\n",
    "print(\"Length of LABEL vocab:\", len(LABEL.vocab))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, valid, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False)\n",
    "\n",
    "print(\"Iterators generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: 25002\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network based on FastText paper\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab, embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        embedded = embedded.permute(1,0,2)\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n",
    "        \n",
    "        return self.fc(pooled)\n",
    "\n",
    "print(\"Input dimensions:\", len(TEXT.vocab))\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to train the model!\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    \n",
    "    # print(y)\n",
    "    correct = (rounded_preds == y).float()\n",
    "    \n",
    "    accuracy = correct.sum()/len(correct)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.stars)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.stars)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.stars)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.stars)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JG\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.225, Train Acc: 90.97%, Val. Loss: 0.189, Val. Acc: 97.09%\n",
      "Epoch: 2, Train Loss: 0.074, Train Acc: 97.80%, Val. Loss: 0.195, Val. Acc: 97.75%\n",
      "Epoch: 3, Train Loss: 0.055, Train Acc: 98.33%, Val. Loss: 0.193, Val. Acc: 98.06%\n",
      "Epoch: 4, Train Loss: 0.046, Train Acc: 98.61%, Val. Loss: 0.198, Val. Acc: 98.28%\n",
      "Epoch: 5, Train Loss: 0.041, Train Acc: 98.75%, Val. Loss: 0.213, Val. Acc: 98.33%\n"
     ]
    }
   ],
   "source": [
    "# Train for a number of epochs\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    print('Epoch: {}, Train Loss: {:.3f}, Train Acc: {:.2f}%, Val. Loss: {:.3f}, Val. Acc: {:.2f}%'.format(epoch + 1, train_loss, train_acc * 100, valid_loss, valid_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy seems quite good, let's test it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JG\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.257, Test Acc: 98.11%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print('Test Loss: {:.3f}, Test Acc: {:.2f}%'.format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it seems like test set accuracy is quite close to train and validation accuracy, indicating that we are probably not overfitting much. Just for fun, let's predict sentiment of some custom reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 1.0000, Text = I love this restaurant!\n",
      "Sentiment: 0.0000, Text = This place is the worst\n",
      "Sentiment: 1.0000, Text = I'll come here again\n",
      "Sentiment: 0.0000, Text = I'll never come here again\n",
      "Sentiment: 0.0000, Text = This place was great but the service was terrible\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = 1.0 - torch.sigmoid(model(tensor))\n",
    "    return prediction.item()\n",
    "\n",
    "sentence1 = \"I love this restaurant!\"\n",
    "sentence2 = \"This place is the worst\"\n",
    "sentence3 = \"I'll come here again\"\n",
    "sentence4 = \"I'll never come here again\"\n",
    "sentence5 = \"This place was great but the service was terrible\"\n",
    "\n",
    "for i in [sentence1, sentence2, sentence3, sentence4, sentence5]:\n",
    "    print(\"Sentiment: {:.4f}, Text = {}\".format(predict_sentiment(i), i))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
